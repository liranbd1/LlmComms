Core Library
- LlmClient executes requests via the composed middleware chain, applies default max-token configuration, and delegates streaming directly to the provider (src/LlmComms.Core/Client/LlmClient.cs).
- Responses expose optional reasoning traces and streaming events surface reasoning chunks when providers supply chain-of-thought output (src/LlmComms.Abstractions/Contracts/Response.cs, StreamEvent.cs).
- LlmClientBuilder captures provider/model selection, clones ClientOptions, and composes the default pipeline of tracing → redaction → logging → metrics → validator → optional cache → terminal middleware (src/LlmComms.Core/Client/LlmClientBuilder.cs, src/LlmComms.Core/Client/MiddlewarePipelineBuilder.cs).
- Abstractions define stable request/response/streaming DTOs, tool definitions, client options, and provider ports to keep adapters loosely coupled (src/LlmComms.Abstractions/Contracts/*, src/LlmComms.Abstractions/Ports/*).

Middleware & Utilities
- TracingMiddleware starts an Activity per call or stream, tagging provider/model/request metadata (src/LlmComms.Core/Middleware/TracingMiddleware.cs).
- RedactionMiddleware stores sanitized message copies and previews using regex rules for downstream telemetry (src/LlmComms.Core/Middleware/RedactionMiddleware.cs).
- LoggingMiddleware emits structured start/success/failure events and rolls up streaming outcomes while reusing redacted previews and request hashes (src/LlmComms.Core/Middleware/LoggingMiddleware.cs).
- MetricsMiddleware pushes OpenTelemetry counters/histograms for request counts, durations, and token usage across unary and streaming flows (src/LlmComms.Core/Middleware/MetricsMiddleware.cs).
- ValidatorMiddleware validates JSON-mode responses and checks tool invocation schemas both synchronously and during streaming (src/LlmComms.Core/Middleware/ValidatorMiddleware.cs).
- CacheMiddleware uses deterministic request hashes, provider hints, and TTL overrides to read/write through a pluggable cache (src/LlmComms.Core/Middleware/CacheMiddleware.cs, src/LlmComms.Core/Utilities/RequestHasher.cs).
- InMemoryCache clones responses for safety and supports TTL-based eviction, backing the cache middleware by default (src/LlmComms.Core/Cache/InMemoryCache.cs).
- HttpClientTransport reflects over anonymous/dynamic request objects to send HTTP calls but is not yet integrated into providers (src/LlmComms.Core/Transport/HttpClientTransport.cs).

Policies
- RetryPolicy implements decorrelated jitter with rate-limit awareness; TimeoutPolicy wraps actions with linked cancellation; CompositePolicy chains policies from outermost to innermost (src/LlmComms.Core/Policies/*).

Tests
- Unit suites cover each middleware’s primary behaviors (redaction, logging, metrics, validation, caching), the in-memory cache, and LlmClient pipeline execution/stream guards (tests/LlmComms.Tests.Unit/*).
- Integration test project exists but has no scenarios yet (tests/LlmComms.Tests.Integration).

Providers & Samples
- Ollama provider implements send/stream paths against the REST API, including reasoning traces and thinking streams (src/LlmComms.Providers.Ollama/*).
- OpenAI, Anthropic, and Azure provider projects remain scaffolds (src/LlmComms.Providers.*).
- Basic sample now demonstrates chatting with an Ollama model and printing reasoning traces (samples/LlmComms.Samples.Basic).

Notable Gaps
- Only the Ollama adapter is implemented; OpenAI/Azure/Anthropic providers remain TODO.
- Streaming options such as ClientOptions.EnableTokenUsageEvents and CoalesceFinalStreamText are defined but unused in the current pipeline.
- Broader integration coverage (beyond the Ollama happy-path smoke test) is still missing.
