Core Library
- LlmClient executes requests via the composed middleware chain, applies default max-token configuration, and delegates streaming directly to the provider (src/LlmComms.Core/Client/LlmClient.cs).
- LlmClientBuilder captures provider/model selection, clones ClientOptions, and composes the default pipeline of tracing → redaction → logging → metrics → validator → optional cache → terminal middleware (src/LlmComms.Core/Client/LlmClientBuilder.cs, src/LlmComms.Core/Client/MiddlewarePipelineBuilder.cs).
- Abstractions define stable request/response/streaming DTOs, tool definitions, client options, and provider ports to keep adapters loosely coupled (src/LlmComms.Abstractions/Contracts/*, src/LlmComms.Abstractions/Ports/*).

Middleware & Utilities
- TracingMiddleware starts an Activity per call or stream, tagging provider/model/request metadata (src/LlmComms.Core/Middleware/TracingMiddleware.cs).
- RedactionMiddleware stores sanitized message copies and previews using regex rules for downstream telemetry (src/LlmComms.Core/Middleware/RedactionMiddleware.cs).
- LoggingMiddleware emits structured start/success/failure events and rolls up streaming outcomes while reusing redacted previews and request hashes (src/LlmComms.Core/Middleware/LoggingMiddleware.cs).
- MetricsMiddleware pushes OpenTelemetry counters/histograms for request counts, durations, and token usage across unary and streaming flows (src/LlmComms.Core/Middleware/MetricsMiddleware.cs).
- ValidatorMiddleware validates JSON-mode responses and checks tool invocation schemas both synchronously and during streaming (src/LlmComms.Core/Middleware/ValidatorMiddleware.cs).
- CacheMiddleware uses deterministic request hashes, provider hints, and TTL overrides to read/write through a pluggable cache (src/LlmComms.Core/Middleware/CacheMiddleware.cs, src/LlmComms.Core/Utilities/RequestHasher.cs).
- InMemoryCache clones responses for safety and supports TTL-based eviction, backing the cache middleware by default (src/LlmComms.Core/Cache/InMemoryCache.cs).
- HttpClientTransport reflects over anonymous/dynamic request objects to send HTTP calls but is not yet integrated into providers (src/LlmComms.Core/Transport/HttpClientTransport.cs).

Policies
- RetryPolicy implements decorrelated jitter with rate-limit awareness; TimeoutPolicy wraps actions with linked cancellation; CompositePolicy chains policies from outermost to innermost (src/LlmComms.Core/Policies/*).

Tests
- Unit suites cover each middleware’s primary behaviors (redaction, logging, metrics, validation, caching), the in-memory cache, and LlmClient pipeline execution/stream guards (tests/LlmComms.Tests.Unit/*).
- Integration test project exists but has no scenarios yet (tests/LlmComms.Tests.Integration).

Providers & Samples
- Provider adapters for OpenAI, Anthropic, and Azure are placeholders with no implementation (src/LlmComms.Providers.*).
- Sample console hosts are default templates without wiring to the client (samples/LlmComms.Samples.*).

Notable Gaps
- No concrete provider adapters, transport wiring, or policy integration means the client cannot reach real LLM endpoints yet.
- Streaming options such as ClientOptions.EnableTokenUsageEvents and CoalesceFinalStreamText are defined but unused in the current pipeline.
- Integration tests and sample applications do not exercise the end-to-end pipeline, leaving observability and provider behavior unverified.
